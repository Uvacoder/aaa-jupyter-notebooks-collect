{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b0a6f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T11:46:11.850736Z",
     "iopub.status.busy": "2021-08-18T11:46:11.849441Z",
     "iopub.status.idle": "2021-08-18T12:07:46.314338Z",
     "shell.execute_reply": "2021-08-18T12:07:46.314892Z",
     "shell.execute_reply.started": "2021-08-18T11:18:25.674792Z"
    },
    "papermill": {
     "duration": 1294.475043,
     "end_time": "2021-08-18T12:07:46.315226",
     "exception": false,
     "start_time": "2021-08-18T11:46:11.840183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 6870 pairs in English. Evaluation: 2945 pairs in English\n",
      "Label distribution in training set: {0: '35.33%', 1: '31.53%', 2: '33.14%'}\n",
      "Run 1. Best epoch: 4 of 20. Training loss: 0.94, validation accuracy: 42.50%, test label distribution: {0: '35.93%', 1: '32.43%', 2: '31.65%'}\n",
      "Run 2. Best epoch: 6 of 20. Training loss: 0.86, validation accuracy: 43.38%, test label distribution: {0: '39.15%', 1: '26.93%', 2: '33.92%'}\n",
      "Run 3. Best epoch: 3 of 20. Training loss: 1.01, validation accuracy: 43.81%, test label distribution: {0: '30.87%', 1: '37.01%', 2: '32.12%'}\n",
      "Run 4. Best epoch: 4 of 20. Training loss: 0.91, validation accuracy: 41.78%, test label distribution: {0: '35.04%', 1: '30.46%', 2: '34.50%'}\n",
      "Run 5. Best epoch: 6 of 20. Training loss: 0.82, validation accuracy: 47.02%, test label distribution: {0: '35.79%', 1: '34.57%', 2: '29.64%'}\n",
      "Run 6. Best epoch: 5 of 20. Training loss: 0.94, validation accuracy: 45.41%, test label distribution: {0: '35.28%', 1: '30.02%', 2: '34.70%'}\n",
      "Run 7. Best epoch: 4 of 20. Training loss: 0.93, validation accuracy: 41.19%, test label distribution: {0: '30.36%', 1: '38.61%', 2: '31.04%'}\n",
      "Run 8. Best epoch: 6 of 20. Training loss: 0.84, validation accuracy: 45.41%, test label distribution: {0: '32.50%', 1: '36.77%', 2: '30.73%'}\n",
      "Run 9. Best epoch: 5 of 20. Training loss: 0.93, validation accuracy: 44.54%, test label distribution: {0: '24.14%', 1: '31.00%', 2: '44.86%'}\n",
      "Ensemble. Cross-validation accuracy: 48.03%, test label distribution: {0: '43.50%', 1: '29.30%', 2: '27.20%'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Initialise the random seeds\n",
    "def random_init(**kwargs):\n",
    "    random.seed(kwargs['seed'])\n",
    "    torch.manual_seed(kwargs['seed'])\n",
    "    torch.cuda.manual_seed(kwargs['seed'])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def normalise(text):\n",
    "    chars = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    text = text.upper()\n",
    "    words=[]\n",
    "    for w in text.strip().split():\n",
    "        if w.startswith('HTTP'):\n",
    "            continue\n",
    "        while len(w)>0 and w[0] not in chars:\n",
    "            w = w[1:]\n",
    "        while len(w)>0 and w[-1] not in chars:\n",
    "            w = w[:-1]\n",
    "        if len(w) == 0:\n",
    "            continue\n",
    "        words.append(w)\n",
    "    text=' '.join(words)\n",
    "    return text\n",
    "\n",
    "def read_vocabulary(train_text, **kwargs):\n",
    "    vocab = dict()\n",
    "    counts = dict()\n",
    "    num_words = 0\n",
    "    for line in train_text:\n",
    "        line = (list(line.strip()) if kwargs['characters'] else line.strip().split())\n",
    "        for char in line:\n",
    "            if char not in vocab:\n",
    "                vocab[char] = num_words\n",
    "                counts[char] = 0\n",
    "                num_words+=1\n",
    "            counts[char] += 1\n",
    "    num_words = 0\n",
    "    vocab2 = dict()\n",
    "    if not kwargs['characters']:\n",
    "        for w in vocab:\n",
    "            if counts[w] >= args['min_count']:\n",
    "                vocab2[w] = num_words\n",
    "                num_words += 1\n",
    "    vocab = vocab2\n",
    "    for word in [kwargs['start_token'],kwargs['end_token'],kwargs['unk_token']]:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = num_words\n",
    "            num_words += 1\n",
    "    return vocab\n",
    "\n",
    "def load_data(premise, hypothesis, targets=None, cv=False, **kwargs):\n",
    "    assert len(premise) == len(hypothesis)\n",
    "    num_seq = len(premise)\n",
    "    max_words = max([len(t) for t in premise+hypothesis])+2\n",
    "    dataset = len(kwargs['vocab'])*torch.ones((2,max_words,num_seq),dtype=torch.long)\n",
    "    labels = torch.zeros((num_seq),dtype=torch.uint8)\n",
    "    idx = 0\n",
    "    utoken_value = kwargs['vocab'][kwargs['unk_token']]\n",
    "    for i,line in tqdm(enumerate(premise),desc='Allocating data memory',disable=(kwargs['verbose']<2)):\n",
    "        words = (list(line.strip()) if kwargs['characters'] else line.strip().split())\n",
    "        if len(words)==0 or words[0] != kwargs['start_token']:\n",
    "            words.insert(0,kwargs['start_token'])\n",
    "        if words[-1] != kwargs['end_token']:\n",
    "            words.append(kwargs['end_token'])\n",
    "        for jdx,word in enumerate(words):\n",
    "            dataset[0,jdx,idx] = kwargs['vocab'].get(word,utoken_value)\n",
    "        line=hypothesis[i]\n",
    "        words = (list(line.strip()) if kwargs['characters'] else line.strip().split())\n",
    "        if len(words)==0 or words[0] != kwargs['start_token']:\n",
    "            words.insert(0,kwargs['start_token'])\n",
    "        if words[-1] != kwargs['end_token']:\n",
    "            words.append(kwargs['end_token'])\n",
    "        for jdx,word in enumerate(words):\n",
    "            dataset[1,jdx,idx] = kwargs['vocab'].get(word,utoken_value)\n",
    "        if targets is not None:\n",
    "            labels[idx] = targets[i]\n",
    "        idx += 1\n",
    "\n",
    "    if cv == False:\n",
    "        return dataset, labels\n",
    "\n",
    "    idx = [i for i in range(num_seq)]\n",
    "    random.shuffle(idx)\n",
    "    trainset = dataset[:,:,idx[0:int(num_seq*(1-kwargs['cv_percentage']))]]\n",
    "    trainlabels = labels[idx[0:int(num_seq*(1-kwargs['cv_percentage']))]]\n",
    "    validset = dataset[:,:,idx[int(num_seq*(1-kwargs['cv_percentage'])):]]\n",
    "    validlabels = labels[idx[int(num_seq*(1-kwargs['cv_percentage'])):]]\n",
    "    return trainset, validset, trainlabels, validlabels\n",
    "\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        #Base variables\n",
    "        self.vocab = kwargs['vocab']\n",
    "        self.in_dim = len(self.vocab)\n",
    "        self.start_token = kwargs['start_token']\n",
    "        self.end_token = kwargs['end_token']\n",
    "        self.unk_token = kwargs['unk_token']\n",
    "        self.characters = kwargs['characters']\n",
    "        self.embed_dim = kwargs['embedding_size']\n",
    "        self.hid_dim = kwargs['hidden_size']\n",
    "        self.n_layers = kwargs['num_layers']\n",
    "        \n",
    "        #Define the embedding layer\n",
    "        self.embed = nn.Embedding(self.in_dim+1,self.embed_dim,padding_idx=self.in_dim)\n",
    "        #Define the lstm layer\n",
    "        self.lstm = nn.LSTM(input_size=self.embed_dim,hidden_size=self.hid_dim,num_layers=self.n_layers)\n",
    "    \n",
    "    def forward(self, inputs, lengths):\n",
    "        #Inputs are size (LxBx1)\n",
    "        #Forward embedding layer\n",
    "        emb = self.embed(inputs)\n",
    "        #Embeddings are size (LxBxself.embed_dim)\n",
    "\n",
    "        #Pack the sequences for GRU\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(emb, lengths)\n",
    "        #Forward the GRU\n",
    "        packed_rec, self.hidden = self.lstm(packed,self.hidden)\n",
    "        #Unpack the sequences\n",
    "        rec, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_rec)\n",
    "        #Hidden outputs are size (LxBxself.hidden_size)\n",
    "        \n",
    "        #Get last embeddings\n",
    "        out = rec[lengths-1,list(range(rec.shape[1])),:]\n",
    "        #Outputs are size (Bxself.hid_dim)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, bsz):\n",
    "        #Initialise the hidden state\n",
    "        weight = next(self.parameters())\n",
    "        self.hidden = (weight.new_zeros(self.n_layers, bsz, self.hid_dim),weight.new_zeros(self.n_layers, bsz, self.hid_dim))\n",
    "\n",
    "    def detach_hidden(self):\n",
    "        #Detach the hidden state\n",
    "        self.hidden=(self.hidden[0].detach(),self.hidden[1].detach())\n",
    "\n",
    "    def cpu_hidden(self):\n",
    "        #Set the hidden state to CPU\n",
    "        self.hidden=(self.hidden[0].detach().cpu(),self.hidden[1].detach().cpu())\n",
    "        \n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        super(Predictor, self).__init__()\n",
    "        self.hid_dim = kwargs['hidden_size']*2\n",
    "        self.out_dim = 3\n",
    "        #Define the output layer and softmax\n",
    "        self.linear = nn.Linear(self.hid_dim,self.out_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self,input1,input2):\n",
    "        #Outputs are size (Bxself.hid_dim)\n",
    "        inputs = torch.cat((input1,input2),dim=1)\n",
    "        out = self.softmax(self.linear(inputs))\n",
    "        return out\n",
    "\n",
    "def train_model(trainset,trainlabels,encoder,predictor,optimizer,criterion,**kwargs):\n",
    "    trainlen = trainset.shape[2]\n",
    "    nbatches = math.ceil(trainlen/kwargs['batch_size'])\n",
    "    total_loss = 0\n",
    "    total_backs = 0\n",
    "    with tqdm(total=nbatches,disable=(kwargs['verbose']<2)) as pbar:\n",
    "        encoder = encoder.train()\n",
    "        for b in range(nbatches):\n",
    "            #Data batch\n",
    "            X1 = trainset[0,:,b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "            mask1 = torch.clamp(len(kwargs['vocab'])-X1,max=1)\n",
    "            seq_length1 = torch.sum(mask1,dim=0)\n",
    "            ordered_seq_length1, dec_index1 = seq_length1.sort(descending=True)\n",
    "            max_seq_length1 = torch.max(seq_length1)\n",
    "            X1 = X1[:,dec_index1]\n",
    "            X1 = X1[0:max_seq_length1]\n",
    "            rev_dec_index1 = list(range(seq_length1.shape[0]))\n",
    "            for i,j in enumerate(dec_index1):\n",
    "                rev_dec_index1[j] = i\n",
    "            X2 = trainset[1,:,b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "            mask2 = torch.clamp(len(kwargs['vocab'])-X2,max=1)\n",
    "            seq_length2 = torch.sum(mask2,dim=0)\n",
    "            ordered_seq_length2, dec_index2 = seq_length2.sort(descending=True)\n",
    "            max_seq_length2 = torch.max(seq_length2)\n",
    "            X2 = X2[:,dec_index2]\n",
    "            X2 = X2[0:max_seq_length2]\n",
    "            rev_dec_index2 = list(range(seq_length2.shape[0]))\n",
    "            for i,j in enumerate(dec_index2):\n",
    "                rev_dec_index2[j] = i\n",
    "            Y = trainlabels[b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "            #Forward pass\n",
    "            encoder.init_hidden(X1.size(1))\n",
    "            embeddings1 = encoder(X1,ordered_seq_length1)\n",
    "            encoder.detach_hidden()\n",
    "            encoder.init_hidden(X2.size(1))\n",
    "            embeddings2 = encoder(X2,ordered_seq_length2)\n",
    "            embeddings1 = embeddings1[rev_dec_index1]\n",
    "            embeddings2 = embeddings2[rev_dec_index2]\n",
    "            posteriors = predictor(embeddings1,embeddings2)\n",
    "            loss = criterion(posteriors,Y)\n",
    "            #Backpropagate\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #Estimate the latest loss\n",
    "            if total_backs == 100:\n",
    "                total_loss = total_loss*0.99+loss.detach().cpu().numpy()\n",
    "            else:\n",
    "                total_loss += loss.detach().cpu().numpy()\n",
    "                total_backs += 1\n",
    "            encoder.detach_hidden()\n",
    "            pbar.set_description(f'Training epoch. Loss {total_loss/(total_backs+1):.2f}')\n",
    "            pbar.update()\n",
    "    return total_loss/(total_backs+1)\n",
    "\n",
    "def evaluate_model(testset,encoder,predictor,**kwargs):\n",
    "    testlen = testset.shape[2]\n",
    "    nbatches = math.ceil(testlen/kwargs['batch_size'])\n",
    "    predictions = np.zeros((testlen,))\n",
    "    with torch.no_grad():\n",
    "        encoder = encoder.eval()\n",
    "        for b in range(nbatches):\n",
    "            #Data batch\n",
    "            X1 = testset[0,:,b*kwargs['batch_size']:min(testlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "            mask1 = torch.clamp(len(kwargs['vocab'])-X1,max=1)\n",
    "            seq_length1 = torch.sum(mask1,dim=0)\n",
    "            ordered_seq_length1, dec_index1 = seq_length1.sort(descending=True)\n",
    "            max_seq_length1 = torch.max(seq_length1)\n",
    "            X1 = X1[:,dec_index1]\n",
    "            X1 = X1[0:max_seq_length1]\n",
    "            rev_dec_index1 = list(range(seq_length1.shape[0]))\n",
    "            for i,j in enumerate(dec_index1):\n",
    "                rev_dec_index1[j] = i\n",
    "            X2 = testset[1,:,b*kwargs['batch_size']:min(testlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "            mask2 = torch.clamp(len(kwargs['vocab'])-X2,max=1)\n",
    "            seq_length2 = torch.sum(mask2,dim=0)\n",
    "            ordered_seq_length2, dec_index2 = seq_length2.sort(descending=True)\n",
    "            max_seq_length2 = torch.max(seq_length2)\n",
    "            X2 = X2[:,dec_index2]\n",
    "            X2 = X2[0:max_seq_length2]\n",
    "            rev_dec_index2 = list(range(seq_length2.shape[0]))\n",
    "            for i,j in enumerate(dec_index2):\n",
    "                rev_dec_index2[j] = i\n",
    "            #Forward pass\n",
    "            encoder.init_hidden(X1.size(1))\n",
    "            embeddings1 = encoder(X1,ordered_seq_length1)\n",
    "            encoder.init_hidden(X2.size(1))\n",
    "            embeddings2 = encoder(X2,ordered_seq_length2)\n",
    "            embeddings1 = embeddings1[rev_dec_index1]\n",
    "            embeddings2 = embeddings2[rev_dec_index2]\n",
    "            posteriors = predictor(embeddings1,embeddings2)\n",
    "            #posteriors = model(X,ordered_seq_length)\n",
    "            estimated = torch.argmax(posteriors,dim=1)\n",
    "            predictions[b*kwargs['batch_size']:min(testlen,(b+1)*kwargs['batch_size'])] = estimated.detach().cpu().numpy()\n",
    "    return predictions\n",
    "        \n",
    "#Arguments\n",
    "args = {\n",
    "    'cv_percentage': 0.1,\n",
    "    'epochs': 20,\n",
    "    'batch_size': 128,\n",
    "    'embedding_size': 16,\n",
    "    'hidden_size': 64,\n",
    "    'num_layers': 1,\n",
    "    'learning_rate': 0.01,\n",
    "    'seed': 0,\n",
    "    'start_token': '<s>',\n",
    "    'end_token': '<\\s>',\n",
    "    'unk_token': '<UNK>',\n",
    "    'verbose': 1,\n",
    "    'characters': False,\n",
    "    'min_count': 15,\n",
    "    'device': torch.device(('cuda:0' if torch.cuda.is_available() else 'cpu'))\n",
    "    }\n",
    "\n",
    "#Read data\n",
    "train_data = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\n",
    "test_data = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')\n",
    "#Extract only English language cases\n",
    "train_data = train_data.loc[train_data['language']=='English']\n",
    "test_data = test_data.loc[test_data['language']=='English']\n",
    "#Extract premises and hypothesis\n",
    "train_premise = [normalise(v) for v in train_data.premise.values]\n",
    "train_hypothesis = [normalise(v) for v in train_data.hypothesis.values]\n",
    "test_premise = [normalise(v) for v in test_data.premise.values]\n",
    "test_hypothesis = [normalise(v) for v in test_data.hypothesis.values]\n",
    "train_targets = train_data.label.values\n",
    "print('Training: {0:d} pairs in English. Evaluation: {1:d} pairs in English'.format(len(train_premise),len(test_premise)))\n",
    "print('Label distribution in training set: {0:s}'.format(str({i:'{0:.2f}%'.format(100*len(np.where(train_targets==i)[0])/len(train_targets)) for i in [0,1,2]})))\n",
    "\n",
    "batch_sizes = [64,128,256]\n",
    "min_counts = [5,15,25]\n",
    "\n",
    "it_idx = 0\n",
    "valid_predictions = dict()\n",
    "test_predictions = dict()\n",
    "valid_accuracies = dict()\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for min_count in min_counts:\n",
    "        args['batch_size'] = batch_size\n",
    "        args['min_count'] = min_count\n",
    "    \n",
    "        random_init(**args)\n",
    "\n",
    "        #Make vocabulary and load data\n",
    "        args['vocab'] = read_vocabulary(train_premise+train_hypothesis, **args)\n",
    "        #print('Vocabulary size: {0:d} tokens'.format(len(args['vocab'])))\n",
    "        trainset, validset, trainlabels, validlabels = load_data(train_premise, train_hypothesis, train_targets, cv=True, **args)\n",
    "        testset, _ = load_data(test_premise, test_hypothesis, None, cv=False, **args)\n",
    "\n",
    "        #Create model, optimiser and criterion\n",
    "        encoder = LSTMEncoder(**args).to(args['device'])\n",
    "        predictor = Predictor(**args).to(args['device'])\n",
    "        optimizer = torch.optim.Adam(list(encoder.parameters())+list(predictor.parameters()),lr=args['learning_rate'])\n",
    "        criterion = nn.NLLLoss(reduction='mean').to(args['device'])\n",
    "\n",
    "        #Train epochs\n",
    "        best_acc = 0.0\n",
    "        for ep in range(1,args['epochs']+1):\n",
    "            loss = train_model(trainset,trainlabels,encoder,predictor,optimizer,criterion,**args)\n",
    "            val_pred = evaluate_model(validset,encoder,predictor,**args)\n",
    "            test_pred = evaluate_model(testset,encoder,predictor,**args)\n",
    "            acc = 100*len(np.where((val_pred-validlabels.numpy())==0)[0])/validset.shape[2]\n",
    "            if acc >= best_acc:\n",
    "                best_acc = acc\n",
    "                best_epoch = ep\n",
    "                best_loss = loss\n",
    "                valid_predictions[it_idx] = val_pred\n",
    "                valid_accuracies[it_idx] = acc\n",
    "                test_predictions[it_idx] = test_pred\n",
    "        print('Run {0:d}. Best epoch: {1:d} of {2:d}. Training loss: {3:.2f}, validation accuracy: {4:.2f}%, test label distribution: {5:s}'.format(it_idx+1,best_epoch,args['epochs'],best_loss,best_acc,str({i:'{0:.2f}%'.format(100*len(np.where(test_pred==i)[0])/len(test_pred)) for i in [0,1,2]})))\n",
    "        it_idx += 1\n",
    "\n",
    "#Do the score combination\n",
    "best_epochs = np.argsort([valid_accuracies[ep] for ep in range(it_idx)])[::-1]\n",
    "val_pred = np.array([valid_predictions[ep] for ep in best_epochs[0:5]])\n",
    "val_pred = np.argmax(np.array([np.sum((val_pred==i).astype(int),axis=0) for i in [0,1,2]]),axis=0)\n",
    "test_pred = np.array([test_predictions[ep] for ep in best_epochs[0:5]])\n",
    "test_pred = np.argmax(np.array([np.sum((test_pred==i).astype(int),axis=0) for i in [0,1,2]]),axis=0)\n",
    "acc = 100*len(np.where((val_pred-validlabels.numpy())==0)[0])/validset.shape[2]\n",
    "print('Ensemble. Cross-validation accuracy: {0:.2f}%, test label distribution: {1:s}'.format(acc,str({i:'{0:.2f}%'.format(100*len(np.where(test_pred==i)[0])/len(test_pred)) for i in [0,1,2]})))\n",
    "#Set all predictions to the majority category\n",
    "df_out = pd.DataFrame({'id': pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')['id'], 'prediction': np.argmax([len(np.where(train_targets==i)[0]) for i in [0,1,2]])})\n",
    "#Set only English language cases to the predicted labels\n",
    "df_out.loc[df_out['id'].isin(test_data['id']),'prediction']=test_pred\n",
    "df_out.to_csv('/kaggle/working/submission.csv'.format(it_idx,acc),index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1304.314223,
   "end_time": "2021-08-18T12:07:48.130974",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-18T11:46:03.816751",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
